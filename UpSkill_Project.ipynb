{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcf9OsE1rBMqVxJgAmsCWe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishesh-tamrakar/Upskill_Quality_Prediction/blob/main/UpSkill_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Project10 : QUALITY PREDICTION IN A MINING PROCESS**\n",
        "---\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LjdEWeR9xQ2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing Libraries"
      ],
      "metadata": {
        "id": "JojX7kO01FIY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4sN-8bEtbbA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "Hc14FwlMt5Y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset was obtained from a mineral processing plant separating silica from iron ore using the reverse cationic flotation method. Continuous process data were collected from 1 a.m. on March 10, 2017 to 11 p.m. on September 9, 2017.\n",
        "\n",
        "Each row of data consists of 23 measurements that can be categorized into four types:\n",
        "\n",
        "\n",
        "* raw materials (column 2-3);\n",
        "* environment variables (column 4-8);\n",
        "* process variables (column 9-22);\n",
        "* processed materials (column 23-24).\n",
        "\n",
        "Raw materials and processed materials were sampled on an hourly basis while the others were sampled every 20 second."
      ],
      "metadata": {
        "id": "VOTPUhtg1uUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = '/content/gdrive/My Drive/UpSkill/'\n",
        "filename = 'MiningProcess_Flotation_Plant_Database.csv'\n",
        "cols_renamed = [\n",
        "    'date',          # Timestamp of measurements, formatted YYYY-MM-DD HH:MM:SS\n",
        "    'feed_iron',     # %Iron (valuables) in the ore being fed into the flotation cell\n",
        "    'feed_silica',   # %Silica (gangue) in the ore being fed into the cell\n",
        "    'starch_flow',   # Amount of starch (reagent) added into the cell, measured in m^3/h\n",
        "    'amina_flow',    # Amount of amina (reagent) added into the cell, measured in m^3/h\n",
        "    'pulp_flow',     # Amount of ore pulp fed into the cell, measured in tonnes/hour\n",
        "    'pulp_ph',       # Acidity/alkalinity of ore pulp on a scale of 0-14\n",
        "    'pulp_density',  # Amount of ore in the pulp, between 1-3 kg/cm^3\n",
        "    'air_col1',      # Volume of air injected into the cell, measured in Nm3/h\n",
        "    'air_col2',\n",
        "    'air_col3',\n",
        "    'air_col4',\n",
        "    'air_col5',\n",
        "    'air_col6',\n",
        "    'air_col7',\n",
        "    'level_col1',    # Froth height in the cell, measured in mm\n",
        "    'level_col2',\n",
        "    'level_col3',\n",
        "    'level_col4',\n",
        "    'level_col5',\n",
        "    'level_col6',\n",
        "    'level_col7',\n",
        "    'conc_iron',     # Lab measurement: %Iron in the end of flotation process\n",
        "    'conc_silica']   # Lab measurement: %Silica in the end of flotation process\n",
        "df = pd.read_csv(\n",
        "    filepath+filename,\n",
        "    header=0,\n",
        "    names=cols_renamed,\n",
        "    parse_dates=['date'],\n",
        "    infer_datetime_format=True,\n",
        "    decimal=',')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "kQfDfCmiuCcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "zbmhITCBuHRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## % SILICA CONCENTRATION FORECAST TIME CALCULATION\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "99BXiowxuJbf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Preprocessing Time-Series Data"
      ],
      "metadata": {
        "id": "kZodWF3Y4BbB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Column date is resampled to reduce the frequency of our time-series data into an hourly basis. This is achieved by selecting only the first measurements of each hour."
      ],
      "metadata": {
        "id": "H_uQFrz44OlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resample data to hourly basis\n",
        "df = df.set_index('date').resample('H').first()\n",
        "df.shape"
      ],
      "metadata": {
        "id": "H0Y2InmpuLgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon inspecting the data, 318 rows containing missing values are found between 2017-03-16 06:00 to 2017-03-29 11:00. Missing values introduce discontinuity in the time-series data and can be detrimental to our forecast. Therefore, only the data starting from 2017-03-29 12:00 will be further used."
      ],
      "metadata": {
        "id": "JTZiPgUK4RzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nans = df[df.isna().any(axis=1)]  # Check for missing values\n",
        "print(f'Total rows with NaNs: {nans.shape[0]}\\n')\n",
        "nans"
      ],
      "metadata": {
        "id": "OPJP04gVuOp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove data with time discontinuity\n",
        "df = df['2017-03-29 12:00:00':]\n",
        "df"
      ],
      "metadata": {
        "id": "7lK01hMNuOyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following plot shows mineral content before (i.e., in the feed) and after flotation process (in the concentrate). As can be observed from the figure, the purpose of flotation is to increase recovery of iron mineral while reducing the gangue (silica).\n",
        "\n",
        "During some periods (e.g., May 13 to June 13), mineral content in the feed was constant but the resulting content in the concentrate fluctuated. This suggests that %iron and %silica in concentrate are not solely governed by the content of raw materials but other parameters as well (i.e., environment, process variables)."
      ],
      "metadata": {
        "id": "UmF8vg_d4ox1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "content = ['feed_iron', 'feed_silica', 'conc_iron', 'conc_silica']\n",
        "palette = ['#FB6542', '#FFBB00', '#3F681C', '#375E97']\n",
        "\n",
        "# Plot mineral content before and after flotation\n",
        "plt.style.use('ggplot')\n",
        "fig, ax = plt.subplots(figsize=(18,6))\n",
        "for pct, color in zip(content, palette):\n",
        "    ax.plot(df.index.values, pct, data=df, color=color)\n",
        "ax.set_title('Mineral content in feed and concentrate',\n",
        "             loc='left', weight='bold', size=16)\n",
        "ax.set_ylabel('% Mineral')\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.legend(loc='center left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oR-1m3cruUDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The column conc_iron is dropped from the dataframe because we want to look at forecasting %silica in concentrate without including %iron in concentrate as a feature. The data are then normalized as they have different units and scales."
      ],
      "metadata": {
        "id": "8zWHHTCA4sWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = list(df)\n",
        "cols.insert(0, cols.pop(         # Moving target `conc_silica` to the front\n",
        "    cols.index('conc_silica')))  # Not necessary but I prefer to do so\n",
        "df = df.loc[:, cols]\n",
        "df.to_csv('./Flotation_Dataset_by_Hour.csv')  # For safekeeping\n",
        "\n",
        "# Drop `conc_iron` then normalize all data\n",
        "values = df.drop('conc_iron', axis=1).values\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled = scaler.fit_transform(values)\n",
        "scaled[0]  # Show first element of the array"
      ],
      "metadata": {
        "id": "KYXVdeMZuWcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The preprocessing stage involves framing the dataset as a supervised learning problem where we forecast the %silica in concentrate at the current hour (t) given the parameters (i.e., raw materials, environment, process) in prior time steps (t-n). We transform the dataset using the series_to_supervised() function below, which was adapted from the blog Machine Learning Mastery by Jason Brownlee."
      ],
      "metadata": {
        "id": "HCQ7M-kh4wue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert series to supervised learning\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "    \"\"\"\n",
        "    Frame a time series as a supervised learning dataset.\n",
        "    Arguments:\n",
        "        data: Sequence of observations as a list or NumPy array.\n",
        "        n_in: Number of lag observations as input (X).\n",
        "        n_out: Number of observations as output (y).\n",
        "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
        "    Returns:\n",
        "        Pandas DataFrame of series framed for supervised learning.\n",
        "    \"\"\"\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df = pd.DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "\n",
        "    for i in range(n_in, 0, -1):   # Input sequence (t-n, ... t-1)\n",
        "        cols.append(df.shift(i))\n",
        "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\n",
        "    for i in range(0, n_out):      # Forecast sequence (t, t+1, ... t+n)\n",
        "        cols.append(df.shift(-i))\n",
        "        if i == 0:\n",
        "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "        else:\n",
        "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\n",
        "    agg = pd.concat(cols, axis=1)  # Put it all together\n",
        "    agg.columns = names\n",
        "    if dropnan:                    # Drop rows with NaN values\n",
        "        agg.dropna(inplace=True)\n",
        "\n",
        "    # Drop columns we don't want to predict\n",
        "    drop_cols = ['var'+str(i)+'(t)' for i in range(2,23)]\n",
        "    agg.drop(columns=drop_cols, axis=1, inplace=True)\n",
        "    return agg"
      ],
      "metadata": {
        "id": "5i0X6tbhuZwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reframed = series_to_supervised(scaled, n_in=1, n_out=1)\n",
        "reframed  # Show reframed dataset"
      ],
      "metadata": {
        "id": "b5fpk8SAucA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Column var1(t-1) until var22(t-1) are our features/inputs. Measured values of the inputs are lagged 1 hour before the target var1(t) (%silica in concentrate at time t).\n",
        "\n",
        "Transformed data is further split into three sets: training (60%), validation (20%), and testing (20%). Flotation data in the period between 2017-03-29 12:00 and 2017-08-08 00:00 is used for training/validation purpose, totaling 3157 hours."
      ],
      "metadata": {
        "id": "7XC1OZ7a44im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = 22             # Number of inputs for forecast\n",
        "n_hours = 1                 # Number of hours with which to lag features\n",
        "n_obs = n_hours*n_features\n",
        "\n",
        "# Define row size of each split\n",
        "n_train = int(np.round(len(reframed)*.60))\n",
        "n_valid = int(np.round(len(reframed)*.20))\n",
        "n_test = int(np.round(len(reframed)*.20))\n",
        "\n",
        "# Split dataset by row size\n",
        "values = reframed.values\n",
        "train = values[:n_train, :]\n",
        "valid = values[n_train:(n_train+n_valid), :]\n",
        "test = values[(n_train+n_valid):, :]\n",
        "\n",
        "# Each set further split into inputs/features (X) and output (y)\n",
        "train_X, train_y = train[:, :n_obs], train[:, -1]\n",
        "valid_X, valid_y = valid[:, :n_obs], valid[:, -1]\n",
        "test_X, test_y = test[:, :n_obs], test[:, -1]\n",
        "\n",
        "# Reshape inputs (X) to be 3D [samples, timesteps, features]\n",
        "train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n",
        "valid_X = valid_X.reshape((valid_X.shape[0], n_hours, n_features))\n",
        "test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n",
        "\n",
        "print(  # Show the final shape of each set\n",
        "    f'Training set  : {train_X.shape}, {train_y.shape}',\n",
        "    f'\\nValidation set: {valid_X.shape}, {valid_y.shape}',\n",
        "    f'\\nTesting set   : {test_X.shape}, {test_y.shape}')"
      ],
      "metadata": {
        "id": "Q61Y7bJ8ueSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Building And Training Model"
      ],
      "metadata": {
        "id": "JXFz9W4C4-Jp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forecasting gangue in flotation concentrate is a time-series related problem. A variation of the recurrent neural networks (RNN), called the long short-term memory (LSTM), is a deep learning approach that can be implemented to solve the problem\n",
        "\n",
        "Model construction and training is done with Keras, a Python deep learning library. The model is made up of two LSTM layers, each with 16 memory cells, and followed by a fully-connected (Dense) layer. In training, the data are grouped into mini batches of 16 training samples. The training process is run for 100 epochs (one epoch is the number of passes needed to complete the entire training samples)."
      ],
      "metadata": {
        "id": "8-ugEKx05Mz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([   # Define a sequential model\n",
        "    LSTM(units=16,\n",
        "         return_sequences=True,\n",
        "         input_shape=(train_X.shape[1],\n",
        "                      train_X.shape[2])),\n",
        "    LSTM(units=16),\n",
        "    Dense(1)\n",
        "])\n",
        "model.compile(\n",
        "    loss='mae',        # Mean absolute error\n",
        "    optimizer='adam')  # Learning rate = 0.001\n",
        "model.summary()        # Display model's architecture"
      ],
      "metadata": {
        "id": "OmIwWAdQugwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(   # Fit on training data\n",
        "    train_X,\n",
        "    train_y,\n",
        "    epochs=100,\n",
        "    batch_size=16,\n",
        "    validation_data=(  # Supply validation data\n",
        "        valid_X,\n",
        "        valid_y),\n",
        "    verbose=2,\n",
        "    shuffle=False,\n",
        "    callbacks=ModelCheckpoint(  # Save model\n",
        "        './LSTM_Flotation_Gangue.hdf5'))"
      ],
      "metadata": {
        "id": "zOvJnnunujYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract losses from training history\n",
        "train_loss = history.history['loss']\n",
        "valid_loss = history.history['val_loss']\n",
        "\n",
        "# Plot learning curves\n",
        "plt.style.use('ggplot')\n",
        "fig, ax = plt.subplots(figsize=(9,6))\n",
        "ax.plot(train_loss, color=palette[0], label='Training')\n",
        "ax.plot(valid_loss, color=palette[2], label='Validation')\n",
        "ax.set_title('Learning Curves', loc='left', weight='bold', size=16)\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Loss (Mean Absolute Error)')\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.legend(loc='center right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r-GvEOKcu3JE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot above indicates that the model exhibits a good fit where training and validation losses decrease to a point of stability and there is minimal gap between the two values."
      ],
      "metadata": {
        "id": "vLYLk5xj56Lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Best training loss   = {min(train_loss):.4f}',\n",
        "      f'at epoch {train_loss.index(min(train_loss))}',\n",
        "      f'\\nBest validation loss = {min(valid_loss):.4f}',\n",
        "      f'at epoch {valid_loss.index(min(valid_loss))}')"
      ],
      "metadata": {
        "id": "IkMxAYELu5l9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Forecasting With LSTM"
      ],
      "metadata": {
        "id": "8hBQr8MB5-PB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forecasting is performed using the trained model and on the data that was not included during training/validation process. Forecasts and actual values are inverted back into their original scales before calculating error scores for the model. Two error metrics are considered in the forecast evaluation: (1) mean absolute error, and (2) root mean squared error."
      ],
      "metadata": {
        "id": "Tq2yhrg36G7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make prediction using test features\n",
        "yhat = model.predict(test_X)\n",
        "\n",
        "# Reshape test data\n",
        "test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))\n",
        "test_y = test_y.reshape((len(test_y), 1))\n",
        "\n",
        "# Invert scaling for forecasts\n",
        "inv_yhat = np.concatenate((yhat, test_X[:, -(n_features-1):]), axis=1)\n",
        "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
        "inv_yhat = inv_yhat[:,0]\n",
        "\n",
        "# Invert scaling for actual values\n",
        "inv_y = np.concatenate((test_y, test_X[:, -(n_features-1):]), axis=1)\n",
        "inv_y = scaler.inverse_transform(inv_y)\n",
        "inv_y = inv_y[:,0]\n",
        "\n",
        "# Calculate error scores\n",
        "mae = mean_absolute_error(inv_y, inv_yhat)\n",
        "rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
        "print(32*'-'+'\\nFORECAST EVALUATION'+'\\n'+32*'-',\n",
        "      f'\\nMean absolute error    : {mae:.4f}',\n",
        "      f'\\nRoot mean squared error: {rmse:.4f}')"
      ],
      "metadata": {
        "id": "o11KwjW0u7RJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define date as x-axis\n",
        "test_date = df.index[-test_y.shape[0]:]\n",
        "\n",
        "# Confidence interval 95% (Z-value 1.96)\n",
        "ci = 1.96*np.std(inv_y)/np.mean(inv_y)\n",
        "\n",
        "# Plot actual values and forecasts\n",
        "plt.style.use('ggplot')\n",
        "fig, ax = plt.subplots(figsize=(18,6))\n",
        "ax.plot(test_date, inv_y, color=palette[1], label='Actual Value')\n",
        "ax.plot(test_date, inv_yhat, color=palette[2], label='Forecast')\n",
        "ax.fill_between(test_date, (inv_y-ci), (inv_y+ci), color=palette[3],\n",
        "                alpha=.1, label='95% Confidence Interval')\n",
        "ax.set_title('%Silica in Concentrate: Actual Values and Forecasts by LSTM',\n",
        "             loc='left', weight='bold', size=16)\n",
        "ax.set_ylabel('Silica in Concentrate (%)')\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YT44f3Eau9XA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A comparison of forecasts and actual values of %silica in concentrate is displayed above for the period between 1 a.m. August 8, 2017 and 11 p.m. September 9, 2017. The forecasts largely follow the pattern of actual values and contained within the 95% confidence interval."
      ],
      "metadata": {
        "id": "wZMaU0r66MUO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Forecast With Random Forest"
      ],
      "metadata": {
        "id": "2z798Q3m6QOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A random forest regressor, in comparison to LSTM, generates greater error and may not perform as well for long-term forecast."
      ],
      "metadata": {
        "id": "WglCAZb26aVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reframe data by lagging features by 1 hour\n",
        "rf_values = df.drop('conc_iron', axis=1).values\n",
        "rf_reframed = series_to_supervised(rf_values, n_in=1, n_out=1)\n",
        "\n",
        "# Define features and target\n",
        "rf_X = rf_reframed.values[:, :-1]\n",
        "rf_y = rf_reframed.values[:, -1]\n",
        "\n",
        "# Split data into train/test sets (80/20)\n",
        "rf_train_X, rf_test_X, rf_train_y, rf_test_y = train_test_split(\n",
        "    rf_X, rf_y, test_size=.20, random_state=0, shuffle=False)\n",
        "\n",
        "# Normalize features\n",
        "rf_scaler = MinMaxScaler(feature_range=(0,1))\n",
        "rf_train_X = rf_scaler.fit_transform(rf_train_X)\n",
        "rf_test_X = rf_scaler.transform(rf_test_X)\n",
        "\n",
        "# Instantiate regressor\n",
        "forest = RandomForestRegressor(random_state=0)\n",
        "\n",
        "# Fit model on training data\n",
        "forest.fit(rf_train_X, rf_train_y)\n",
        "\n",
        "# Make prediction using trained model\n",
        "rf_yhat = forest.predict(rf_test_X)\n",
        "\n",
        "# Calculate error scores\n",
        "rf_mae = mean_absolute_error(rf_test_y, rf_yhat)\n",
        "rf_rmse = np.sqrt(mean_squared_error(rf_test_y, rf_yhat))\n",
        "print(32*'-'+'\\nFORECAST EVALUATION'+'\\n'+32*'-',\n",
        "      f'\\nMean absolute error    : {rf_mae:.4f}',\n",
        "      f'\\nRoot mean squared error: {rf_rmse:.4f}')"
      ],
      "metadata": {
        "id": "bTPMFAV7u_qK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot actual values and forecasts\n",
        "plt.style.use('ggplot')\n",
        "fig, ax = plt.subplots(figsize=(18,6))\n",
        "ax.plot(test_date, rf_test_y, color=palette[1], label='Actual Value')\n",
        "ax.plot(test_date, rf_yhat, color=palette[2], label='Forecast')\n",
        "ax.fill_between(test_date, (rf_test_y-ci), (rf_test_y+ci), color=palette[3],\n",
        "                alpha=.1, label='95% Confidence Interval')\n",
        "ax.set_title('%Silica in Concentrate: Actual Values and Forecasts by Random Forest',\n",
        "             loc='left', weight='bold', size=16)\n",
        "ax.set_ylabel('Silica in Concentrate (%)')\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bM5hBu4kvDsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "rWtE2MiLwRiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CONCLUSION"
      ],
      "metadata": {
        "id": "wwwVyQs36rbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " deep learning approach using LSTM was implemented to forecast gangue content in flotation concentrate. Excluding %iron in concentrate from the features, %silica in concentrate were forecasted one hour ahead and with error below 1 (based on RMSE, MAE). As the dataset owner stated in this post, MAE and RMSE of 1Â±0.2 is a satisfactory result. The forecasts thus show a promising method for process engineers to make timely assessment of concentrate purity and take corrective actions in advance, especially when purity deviates from the acceptable values.\n",
        "\n",
        "Finally, although LSTM implementation in this notebook has met the objectives, it will benefit from further exploration:\n",
        "\n",
        "\n",
        "\n",
        "* Forecasting with smaller lag timesteps. For example, a 30-minute lag for the features/inputs.\n",
        "* Analysis of feature importance in order to understand which parameters of the flotation process greatly affect %silica in concentrate. This ensures that the important parameters are adjusted accordingly.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Tv0be_0W6pjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PREDICTION OF % SILICA CONCENTRATE WITH AND WITHOUT USING % IRON CONCENTRATE COLUMN\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gMz9P1kcwUkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/gdrive/My Drive/UpSkill/MiningProcess_Flotation_Plant_Database.csv')"
      ],
      "metadata": {
        "id": "a6jLUteWwXCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isna().sum()"
      ],
      "metadata": {
        "id": "sSfUSV5dwjxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Preprocessing The Data"
      ],
      "metadata": {
        "id": "6zeFFswW8Oqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for column in data.columns:\n",
        "    data[column] = data[column].apply(lambda x: x.replace(',', '.'))"
      ],
      "metadata": {
        "id": "VNF11dAnwl9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "BWYguPK2wome"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Feature Engineering"
      ],
      "metadata": {
        "id": "XAAwQqUD9Wqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['date'] = data['date'].apply(lambda x: re.search('[0-9]*-[0-9]*', x).group(0))\n",
        "\n",
        "data['Year'] = data['date'].apply(lambda x: re.search('^[^-]*', x).group(0))\n",
        "data['Month'] = data['date'].apply(lambda x: re.search('[^-]*$', x).group(0))\n",
        "\n",
        "data = data.drop('date', axis=1)"
      ],
      "metadata": {
        "id": "uBmbdTfGwqpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.astype(np.float)"
      ],
      "metadata": {
        "id": "N_b7kV3nwtWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Visualization"
      ],
      "metadata": {
        "id": "wEtkVhiO9eCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Year'].unique()"
      ],
      "metadata": {
        "id": "w0_O5IEhwyRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop('Year', axis=1)"
      ],
      "metadata": {
        "id": "Z_w9suI8wz6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(data.corr(), vmin=-1.0, vmax=1.0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YlpFgHWMw2I7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here We can clearly see that % Silica Concentration and % Iron Concentration are highly correlated"
      ],
      "metadata": {
        "id": "5sKs_cII-ABU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Splitting And Scaling"
      ],
      "metadata": {
        "id": "j6dhi5Ry9h1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target = '% Silica Concentrate'\n",
        "\n",
        "y = data[target]\n",
        "\n",
        "X_n = data.drop([target, '% Iron Concentrate'], axis=1)\n",
        "X_i = data.drop(target, axis=1)"
      ],
      "metadata": {
        "id": "NHjKsJ4mw4Qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "X_n = scaler.fit_transform(X_n)\n",
        "X_i = scaler.fit_transform(X_i)"
      ],
      "metadata": {
        "id": "0URxyZvFw5yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_n_train, X_n_test, y_n_train, y_n_test = train_test_split(X_n, y, train_size=0.7)\n",
        "X_i_train, X_i_test, y_i_train, y_i_test = train_test_split(X_i, y, train_size=0.7)"
      ],
      "metadata": {
        "id": "QMrK-oOtw7Rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training"
      ],
      "metadata": {
        "id": "-QT1AgRZ9mqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_n = LinearRegression()\n",
        "model_i = LinearRegression()"
      ],
      "metadata": {
        "id": "XWa2N5x4w83q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_n.fit(X_n_train, y_n_train)\n",
        "print(\"Model without iron R^2 Score:\", model_n.score(X_n_test, y_n_test))"
      ],
      "metadata": {
        "id": "YZ1Cud7Fw-ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_i.fit(X_i_train, y_i_train)\n",
        "print(\"Model with iron R^2 Score:\", model_i.score(X_i_test, y_i_test))"
      ],
      "metadata": {
        "id": "cC1Ip5g7xAF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "VBX4euJ3xL5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CONCLUSION"
      ],
      "metadata": {
        "id": "hajCwPrH9qxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, It is possible tp predict % Silica in Concentrate without using % Iron Concentrate column.\n",
        "But here we also observe that the R^2 value with inclusion of % Iron Concentrate column is greater that when not included."
      ],
      "metadata": {
        "id": "L9BVo-fr9tvb"
      }
    }
  ]
}